# Cognitive Neuroscience

* [Biologically plausible neural computation](https://www.sciencedirect.com/science/article/pii/0303264796016255)
* [Summation and multiplication: two distinct operation domains of leaky integrate-and-fire neurons](https://www.tandfonline.com/doi/abs/10.1088/0954-898X_2_4_010)
* [The Computational Cognitive Neuroscience of Learning and Memory: Principles and Models](https://www.sciencedirect.com/science/article/pii/S0166411508100085)
* [Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004792)
* [Task representations in neural networks trained to perform many cognitive tasks](https://www.nature.com/articles/s41593-018-0310-2)
* [Probabilistic Decision Making by Slow Reverberation in Cortical Circuits](https://www.sciencedirect.com/science/article/pii/S0896627302010929)
* [Reward-based training of recurrent neural networks for cognitive and value-based tasks](https://elifesciences.org/articles/21492)

# Natural Language Processing

* [Variational Inference for Grammar Induction with Prior Knowledge](https://www.aclweb.org/anthology/P09-2001.pdf)
* [A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING](https://arxiv.org/pdf/1703.03130.pdf)
* [You Only Need Attention to Traverse Trees](https://www.aclweb.org/anthology/P19-1030.pdf)
* [Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/abs/1603.08983)
* [Assessing the ability of lstms to learn syntax-sensitive dependencies](https://www.aclweb.org/anthology/Q16-1037)
* [Comparative Study of CNN and RNN for Natural Language Processing](https://arxiv.org/abs/1702.01923)
* [Exploring the Syntactic Abilities of RNNs with Multi-task Learning](https://www.aclweb.org/anthology/K17-1003.pdf)
* [Colorless green recurrent networks dream hierarchically](https://www.aclweb.org/anthology/N18-1108)
* [NEURAL LANGUAGE MODELING BY JOINTLY LEARNING SYNTAX AND LEXICON](https://arxiv.org/abs/1711.02013)
* [ORDERED NEURONS: INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS](https://arxiv.org/abs/1810.09536)
* [Quantity doesnâ€™t buy quality syntax with neural language models](https://arxiv.org/abs/1909.00111)


# Machine Learning

* [Practical Variational Inference for Neural Networks](https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks)
* [Bayesian Recurrent Neural Networks](https://arxiv.org/abs/1704.02798)
* [Differentiable quantization of deep networks](https://arxiv.org/abs/1905.11452)
* [End-To-End Memory Networks](https://arxiv.org/abs/1503.08895)
* [Information Theoretic Inequalities](https://pdfs.semanticscholar.org/7db0/752f20b0ffe5f0b25455dd71efc000bb0b60.pdf)
* [Mutual Information Neural Estimator](https://arxiv.org/abs/1801.04062)
* [Recurrent Independent Mechanisms](https://arxiv.org/abs/1909.10893)
* [Residual Networks Behave Like Ensembles of Relatively Shallow Networks](https://arxiv.org/abs/1605.06431)
* [Recurrent Dropout without Memory Loss](https://arxiv.org/abs/1603.05118)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [A Theoretically Grounded Application of Dropout in Recurrent Neural Network](https://arxiv.org/abs/1512.05287)
* [Independent Component Analysis](https://www.sciencedirect.com/science/article/pii/S0893608000000265)